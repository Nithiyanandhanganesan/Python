Next to Matplotlib and NumPy, Pandas is one of the most widely used Python libraries in data science.

Pandas data frame consists of three main components: the data, the index, and the columns.

import pandas as pd

data = np.array([['','Col1','Col2'],
['Row1',1,2],
['Row2',3,4]])
                
print(pd.DataFrame(data=data[1:,1:],
index=data[1:,0],
columns=data[0,1:]))

# Take a 2D array as input to your DataFrame 

my_2darray = np.array([[1, 2, 3], [4, 5, 6]])

print(pd.DataFrame(my_2darray))



# Take a dictionary as input to your DataFrame 

my_dict = {1: ['1', '3'], 2: ['1', '2'], 3: ['2', '4']}

print(pd.DataFrame(my_dict))



# Take a DataFrame as input to your DataFrame 

my_df = pd.DataFrame(data=[4,5,6,7], index=range(0,4), columns=['A'])

print(pd.DataFrame(my_df))

# Take a Series as input to your DataFrame

my_series = pd.Series({"United Kingdom":"London", "India":"New Delhi", "United States":"Washington", "Belgium":"Brussels"})

print(pd.DataFrame(my_series))

# Use the `shape` property

print(df.shape)



# Or use the `len()` function with the `index` property

print(len(df.index))

SELECTING DATA FROM DATAFRAME:
===================================

   A  B  C
0  1  2  3
1  4  5  6
2  7  8  9

# Using `loc[]`

print(df.loc[0]['A'])


Using `get_value(index, column)`

print(df.get_value(0, 'A'))

# Use `iloc[]` to select row `0`

print(df.iloc[0])



# Use `loc[]` to select column `'A'`

print(df.loc[:,'A'])

loc works on labels of your index. This means that if you give in loc[2], you look for the values of your DataFrame that have an index labeled 2.
iloc works on the positions in your index. This means that if you give in iloc[2], you look for the values of your DataFrame that are at index ’2`.

df.head()
df.tail()
df.index
df.columns
df.values
df.describe()

ADD ROW TO DATAFRAME:
============================

df = pd.DataFrame(data=np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]), columns=['A', 'B', 'C'])



# Use `.index`
df['D'] = df.index



# Print `df`
print(df)

           A  B  C  D
    
	0  1  2  3  0
    
	1  4  5  6  1
   
	2  7  8  9  2


DELETE ROW TO DATAFRAME:
=============================

- resetting the index of your DataFrame (go back to the previous section to see how it is done) or
- remove the index name, if there is any, by executing del df.index.name,
- remove duplicate index values by resetting the index, dropping the duplicates of the index column that has been added to your 
  DataFrame and reinstating that duplicateless column again as the index:

REMOVE INDEX:
-----------------

df = pd.DataFrame(data=np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9], [40, 50, 60], [23, 35, 37]]), index= [2.5, 12.6, 4.8, 4.8, 2.5],columns=[48, 49, 50])
                  

print(df.reset_index().drop_duplicates(subset='index', keep='last').set_index('index'))



# Drop the column with label 'A'                  

print(df.drop('A', axis=1, inplace=False))

# Drop the column at position 1

print(df.drop(df.columns[[1]], axis=1))

The axis argument is either 0 when it indicates rows and 1 when it is used to drop columns.
You can set inplace to True to delete the column without having to reassign the DataFrame.


REMOVE ROW:
--------------

df.drop_duplicates([48], keep='last')
remove duplicate rows from your DataFrame by executing df.drop_duplicates()


If there is no uniqueness criterion to the deletion that you want to perform, you can use the drop() method, where you use the 
index property to specify the index of which rows you want to remove from your DataFrame.

print(df.drop(df.index[1]))


RENAME COLUMNS:
=====================

# Define the new names of your columns
newcols = {
    'A': 'new_column_1', 
    'B': 'new_column_2', 
    'C': 'new_column_3'
}



# Use `rename()` to rename your columns

print(df.rename(columns=newcols, inplace=False))



# Rename your index

print(df.rename(index={1: 'a'}))


REPLACE STRING IN DATAFRAME:
=============================

# Replace the strings by numerical values (0-4)

print(df.replace(['Awful', 'Poor', 'OK', 'Acceptable', 'Perfect'], [0, 1, 2, 3, 4]) )

# Replace strings by others with `regex`

print(df.replace({'\n': '<br>'}, regex=True))

# Delete unwanted parts from the strings in the `result` column

df['result'] = df['result'].map(lambda x: x.lstrip('+-').rstrip('aAbBcC'))

# Make it a Series
# Stack the values

ticket_series = df['Ticket'].str.split(' ').apply(pd.Series, 1).stack()


print(ticket_series)



# Get rid of the stack:

# Drop the level to line up with the DataFrame

ticket_series.index = ticket_series.index.droplevel(-1)


print(ticket_series)



# Make your series a dataframe 
ticketdf = pd.DataFrame(ticket_series)



# Delete the `Ticket` column from your DataFrame

del df['Ticket']



# Join the ticket DataFrame to `df`

df.join(ticketdf)



Applying A Function to Your Pandas DataFrame’s Columns or Rows:
===============================================================

doubler = lambda x: x*2

df['A'].apply(doubler)   ==> apply to row or columns
df['A'].applymap(doubler)   ==> apply the function at element wise

FILE READ:
=====================

pd.read_csv('yourFile', parse_dates=True)
pd.read_csv('yourFile', parse_dates=['columnName'])


dateparser = lambda x: pd.datetime.strptime(x, '%Y-%m-%d %H:%M:%S')

# Which makes your read command:
pd.read_csv(infile, parse_dates=['columnName'], date_parser=dateparse)

# Or combine two columns into a single DateTime column
pd.read_csv(infile, parse_dates={'datetime': ['date', 'time']}, date_parser=dateparse)

df.to_csv('myDataFrame.csv')
df.to_csv('myDataFrame.csv', sep='\t')


CHANGE DATAFRAME STRUCTURE:
===============================

people = pd.DataFrame({'FirstName' : ['John', 'Jane'],
'LastName' : ['Doe', 'Austen'],
'BloodType' : ['A-', 'B+'],'Weight' : [90, 64]})
print(people)



# Use `melt()` on the `people` DataFrame

print(pd.melt(people, id_vars=['FirstName', 'LastName'], var_name='measurements'))


iteration in DATAFRAME:
===========================

df = pd.DataFrame(data=np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]), columns=['A', 'B', 'C'])



for index, row in df.iterrows() :
    
     print(row['A'], row['B'])


CONCATENATION:
======================

#DF1,DF2,DF3 get concatenated as rows
frames = [df1, df2, df3]
result = pd.concat(frames)


pd.concat(objs, axis=0, join='outer', join_axes=None, ignore_index=False,keys=None, levels=None, names=None, verify_integrity=False,copy=True)

-> axis : {0, 1, ...}, default 0. The axis to concatenate along.
-> join : {‘inner’, ‘outer’}, default ‘outer’. How to handle indexes on other axis(es). Outer for union and inner for intersection.
-> join_axes : list of Index objects. Specific indexes to use for the other n - 1 axes instead of performing inner/outer set logic.
-> ignore_index : boolean, default False. If True, do not use the index values on the concatenation axis. The resulting axis will be labeled 0, ..., n - 1
-> keys : sequence, default None. Construct hierarchical index using the passed keys as the outermost level. If multiple levels passed, should contain tuples.
-> levels : list of sequences, default None. Specific levels (unique values) to use for constructing a MultiIndex. Otherwise they will be inferred from the keys.
-> names : list, default None. Names for the levels in the resulting hierarchical index.
-> verify_integrity : boolean, default False. Check whether the new concatenated axis contains duplicates. This can be very expensive relative to the actual data concatenation.
-> copy : boolean, default True. If False, do not copy data unnecessarily.

#similar to concat
result = df1.append([df2, df3])


MERGE:
==========

Similar to SQL join.

pd.merge(left, right, how='inner', on=None, left_on=None, right_on=None,left_index=False, right_index=False, sort=True,suffixes=('_x', '_y'), copy=True, 
         indicator=False)

-> left: A DataFrame object
-> right: Another DataFrame object
-> how: One of 'left', 'right', 'outer', 'inner'. Defaults to inner. See below for more detailed description of each method
-> on: Columns (names) to join on. Must be found in both the left and right DataFrame objects. 
       If not passed and left_index and right_index are False, the intersection of the columns in the DataFrames will be inferred to be the join keys
-> left_on: Columns from the left DataFrame to use as keys. Can either be column names or arrays with length equal to the length of the DataFrame
-> right_on: Columns from the right DataFrame to use as keys. Can either be column names or arrays with length equal to the length of the DataFrame
-> left_index: If True, use the index (row labels) from the left DataFrame as its join key(s). 
   In the case of a DataFrame with a MultiIndex (hierarchical), the number of levels must match the number of join keys from the right DataFrame
-> right_index: Same usage as left_index for the right DataFrame
-> sort: Sort the result DataFrame by the join keys in lexicographical order. Defaults to True, setting to False will improve performance substantially in many cases
-> suffixes: A tuple of string suffixes to apply to overlapping columns. Defaults to ('_x', '_y').
-> copy: Always copy data (default True) from the passed DataFrame objects, even when reindexing is not necessary. 
-> indicator: Add a column to the output DataFrame called _merge with information on the source of each row. 
             _merge is Categorical-type and takes on a value of left_only for observations whose merge key only appears in 'left' DataFrame, 
             right_only for observations whose merge key only appears in 'right' DataFrame, and both if the observation’s merge key is found in both.